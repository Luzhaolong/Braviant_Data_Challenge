{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import linear_model, datasets\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/luke/Desktop/braviant_datachallenge/')\n",
    "x_train = pd.read_csv('x_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv',header=None)\n",
    "x_test = pd.read_csv('x_test.csv')\n",
    "y_test = pd.read_csv('y_test.csv',header= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.drop(['Unnamed: 0'],axis = 1)\n",
    "x_test = x_test.drop(['Unnamed: 0'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_origin_colnames = list(x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.iloc[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ada_params = {\n",
    "#    'n_estimators': [800,1000,1200],\n",
    "#    'learning_rate' : [0.5,0.8]\n",
    "#}\n",
    "#%%time\n",
    "\n",
    "#mdl = AdaBoostClassifier()\n",
    "\n",
    "#grid = GridSearchCV(mdl, ada_params,\n",
    "#                    verbose=1,\n",
    "#                    cv=5,\n",
    "#                    n_jobs=2,\n",
    "#                   scoring='roc_auc')\n",
    "#grid.fit(x_train, y_train)\n",
    "\n",
    "#print(grid.best_params_)\n",
    "#print(grid.best_score_)\n",
    "\n",
    "#rf_params = {\n",
    "#    'n_jobs': [-1],\n",
    "#    'n_estimators': [500,700],\n",
    "#     'warm_start': [True], \n",
    "     #'max_features': [0.2,0.5],\n",
    "#    'max_depth': [2,6,8],\n",
    "#    'min_samples_leaf': [2,6,10],\n",
    "#    'max_features' : ['sqrt'],\n",
    "#    'verbose': [0]\n",
    "#}\n",
    "#%%time\n",
    "#mdl = RandomForestClassifier()\n",
    "#grid = GridSearchCV(mdl, rf_params,\n",
    "#                    verbose=1,\n",
    "#                    cv=5,\n",
    "#                    n_jobs=2,\n",
    "#                   scoring='roc_auc')\n",
    "#grid.fit(x_train, y_train)\n",
    "\n",
    "#print(grid.best_params_)\n",
    "#print(grid.best_score_)\n",
    "\n",
    "#gb_params = {\n",
    "#    'n_estimators': [1000],\n",
    "     #'max_features': 0.2,\n",
    "#    'max_depth': [10],\n",
    "#    'min_samples_leaf': [2],\n",
    "#    'verbose': [0]\n",
    "#}\n",
    "\n",
    "#%%time\n",
    "#mdl = GradientBoostingClassifier()\n",
    "#grid = GridSearchCV(mdl, gb_params,\n",
    "#                    verbose=1,\n",
    "#                    cv=5,\n",
    "#                    n_jobs=2,\n",
    "#                   scoring='roc_auc')\n",
    "#grid.fit(x_train, y_train)\n",
    "#print(grid.best_params_)\n",
    "#print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.values\n",
    "x_test = x_test.values\n",
    "y_train = y_train.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use OOP to make modeling,stacking pipline handy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = x_train.shape[0]\n",
    "ntest = x_test.shape[0]\n",
    "SEED = 0 \n",
    "NFOLDS = 5 \n",
    "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n",
    "\n",
    "class modelHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    ''' we use proba to scoring each case for dense evaluation(from categorical to continious)'''\n",
    "    def predict_proba(self, x):\n",
    "        return self.clf.predict_proba(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        #oof_train[test_index] = clf.predict(x_te)\n",
    "        #oof_test_skf[i, :] = clf.predict(x_test)\n",
    "        \n",
    "        \n",
    "        oof_train[test_index] = clf.predict_proba(x_te)[:,1]\n",
    "        oof_test_skf[i, :] = clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 700,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 1200,\n",
    "    'learning_rate' : 0.8\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = modelHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "ada = modelHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "gb = modelHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round1: scoring using different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    }
   ],
   "source": [
    "rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\n",
    "ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \n",
    "gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>AdaBoost</th>\n",
       "      <th>GradientBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.825230</td>\n",
       "      <td>0.501969</td>\n",
       "      <td>0.999603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.939479</td>\n",
       "      <td>0.501963</td>\n",
       "      <td>0.994406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.474789</td>\n",
       "      <td>0.500101</td>\n",
       "      <td>0.311726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.323611</td>\n",
       "      <td>0.500234</td>\n",
       "      <td>0.908021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.861045</td>\n",
       "      <td>0.501301</td>\n",
       "      <td>0.995707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RandomForest  AdaBoost  GradientBoost\n",
       "0      0.825230  0.501969       0.999603\n",
       "1      0.939479  0.501963       0.994406\n",
       "2      0.474789  0.500101       0.311726\n",
       "3      0.323611  0.500234       0.908021\n",
       "4      0.861045  0.501301       0.995707"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n",
    "     'AdaBoost': ada_oof_train.ravel(),\n",
    "      'GradientBoost': gb_oof_train.ravel()\n",
    "    })\n",
    "base_predictions_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reserve a full variable dataframe for lr\n",
    "x_train_use = x_train\n",
    "x_test_use = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate(( rf_oof_train, ada_oof_train, gb_oof_train), axis=1)\n",
    "x_test = np.concatenate(( rf_oof_test, ada_oof_test, gb_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_use = np.concatenate((x_train_use,x_train),axis =1)\n",
    "x_test_use = np.concatenate((x_test_use,x_test),axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round2: Either lgbm(performance base) or LR(business base) then optional(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameters to search\n",
    "lgbm_params = {\n",
    "    'learning_rate': [0.3],\n",
    "    'n_estimators': [400],\n",
    "    'num_leaves': [2],\n",
    "    'boosting_type' : ['dart'],\n",
    "    'objective' : ['binary'],\n",
    "    'random_state' : [666], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.5],\n",
    "    'subsample' : [0.3],\n",
    "    'reg_alpha' : [0.35],\n",
    "    'reg_lambda' : [0.5],\n",
    "    'silent' : ['True']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "{'boosting_type': 'dart', 'colsample_bytree': 0.5, 'learning_rate': 0.3, 'n_estimators': 400, 'num_leaves': 2, 'objective': 'binary', 'random_state': 666, 'reg_alpha': 0.35, 'reg_lambda': 0.5, 'silent': 'True', 'subsample': 0.3}\n",
      "0.9750595128597904\n",
      "CPU times: user 1.49 s, sys: 18.9 ms, total: 1.51 s\n",
      "Wall time: 5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 out of   5 | elapsed:    4.8s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mdl = lgb.LGBMClassifier()\n",
    "\n",
    "grid= GridSearchCV(mdl, lgbm_params,\n",
    "                    verbose=1,\n",
    "                    cv=5,\n",
    "                    n_jobs=4,\n",
    "                   scoring='roc_auc')\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_test_lgbm = grid.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9362627632206008"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test.iloc[:,1], y_test_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameters to search\n",
    "lr_params = {\n",
    "    'penalty': ['l1','l2'],\n",
    "    'C': np.logspace(0,4,10),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "{'C': 7.742636826811269, 'penalty': 'l1'}\n",
      "0.9733913357526235\n",
      "CPU times: user 111 ms, sys: 73.4 ms, total: 184 ms\n",
      "Wall time: 1.95 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.8s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mdl = linear_model.LogisticRegression()\n",
    "\n",
    "grid = GridSearchCV(mdl, lr_params,\n",
    "                    verbose=1,\n",
    "                    cv=5,\n",
    "                    n_jobs=4,\n",
    "                   scoring='roc_auc')\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_lr = grid.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9340675434657613"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test.iloc[:,1], y_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a original variable dataframe with different algorithm scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_use_x_train = np.concatenate((x_train_use,x_train),axis = 1)\n",
    "lr_use_x_test = np.concatenate((x_test_use,x_test),axis = 1)\n",
    "lr_use_x_train = pd.DataFrame(lr_use_x_train)\n",
    "lr_use_x_test = pd.DataFrame(lr_use_x_test)\n",
    "x_origin_colnames.extend(('sc1','sc2','sc3'))\n",
    "lr_use_x_train.columns = [x_origin_colnames]\n",
    "lr_use_x_test.columns = [x_origin_colnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_explain = linear_model.LogisticRegression()\n",
    "lr_explain.fit(lr_use_x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.30600451e-04,  1.78333076e-01, -3.13424073e-03, -8.80246200e-01,\n",
       "       -1.26461457e-01,  1.20681348e-02, -1.03453592e-02,  9.35449023e-02,\n",
       "       -4.52074466e-02,  2.40393043e+00, -9.02180780e-01,  3.27031468e+00,\n",
       "        2.40393043e+00, -9.02180780e-01,  3.27031468e+00])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_explain.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameter_tbl = pd.DataFrame( {'Feature':x_origin_colnames ,\n",
    "     'Parameter': lr_explain.coef_[0]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Parameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sc2</td>\n",
       "      <td>-0.902181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sc2</td>\n",
       "      <td>-0.902181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x24</td>\n",
       "      <td>-0.880246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x25</td>\n",
       "      <td>-0.126461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bin_positive</td>\n",
       "      <td>-0.045207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>x27</td>\n",
       "      <td>-0.010345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x2</td>\n",
       "      <td>-0.003134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x0</td>\n",
       "      <td>-0.000731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>x26</td>\n",
       "      <td>0.012068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bin_negative</td>\n",
       "      <td>0.093545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x1</td>\n",
       "      <td>0.178333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sc1</td>\n",
       "      <td>2.403930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sc1</td>\n",
       "      <td>2.403930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sc3</td>\n",
       "      <td>3.270315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sc3</td>\n",
       "      <td>3.270315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Feature  Parameter\n",
       "10           sc2  -0.902181\n",
       "13           sc2  -0.902181\n",
       "3            x24  -0.880246\n",
       "4            x25  -0.126461\n",
       "8   bin_positive  -0.045207\n",
       "6            x27  -0.010345\n",
       "2             x2  -0.003134\n",
       "0             x0  -0.000731\n",
       "5            x26   0.012068\n",
       "7   bin_negative   0.093545\n",
       "1             x1   0.178333\n",
       "9            sc1   2.403930\n",
       "12           sc1   2.403930\n",
       "11           sc3   3.270315\n",
       "14           sc3   3.270315"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parameter_tbl.sort_values(by=['Parameter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_lr_explain = lr_explain.predict(lr_use_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.933734650377313"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test.iloc[:,1], y_test_lr_explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.94 s, sys: 353 ms, total: 5.29 s\n",
      "Wall time: 3.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer, init,reg):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, kernel_initializer=init, kernel_regularizer=regularizers.l2(reg),input_shape=(3,)))\n",
    "    model.add(layers.Dense(64, kernel_initializer=init, kernel_regularizer=regularizers.l2(reg),activation='relu'))\n",
    "    model.add(layers.Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=['accuracy'])\n",
    "    return(model)\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = [ 'adam']\n",
    "init = ['normal']\n",
    "reg = [0.01]\n",
    "epochs = [20]\n",
    "batches = [500]\n",
    "\n",
    "param_grid = dict(optimizer = optimizers, init = init,epochs = epochs,batch_size = batches,reg =reg)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='roc_auc')\n",
    "grid_result = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.971674 using {'batch_size': 500, 'epochs': 20, 'init': 'normal', 'optimizer': 'adam', 'reg': 0.01}\n",
      "0.971674 (0.001948) with: {'batch_size': 500, 'epochs': 20, 'init': 'normal', 'optimizer': 'adam', 'reg': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_nn = grid.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9327408041396494"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test.iloc[:,1], y_test_nn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
